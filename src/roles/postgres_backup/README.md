# Ansible Role: Postgres Backup

**Table of Contents**

* [Overview](#overview)
* [Supported Operating Systems/Platforms](#supported-operating-systemsplatforms)
* [Role Variables](#role-variables)
* [Tags](#tags)
* [Dependencies](#dependencies)
* [Example Playbook](#example-playbook)
* [Testing Instructions](#testing-instructions)
* [Known Issues and Gotchas](#known-issues-and-gotchas)
* [Security Implications](#security-implications)
* [Cross-Referencing](#cross-referencing)

## Overview

The **Postgres Backup** role automates the backup of a PostgreSQL database by installing a backup script and scheduling it to run regularly on the host. This role is **general-purpose** – it can be used to back up any specified PostgreSQL database (for example, an application’s database or an entire cluster’s primary database) on a server. It accomplishes this by using the standard PostgreSQL dump utility (`pg_dump`) and compressing the output, all orchestrated via a Systemd service and timer.

Key features of this role include:

* **Database Dump to Compressed File:** Utilizes `pg_dump` to export the target PostgreSQL database, piping the output through **gzip** compression. The result is a `.sql.gz` file that contains the SQL dump of the database, significantly reducing storage space. For example, a database named **`myapp`** will be backed up to a file like `myapp-<DATE>.sql.gz` in the backup directory.
* **Automated Scheduling (Systemd Timer):** Deploys a Systemd **oneshot** service and a corresponding **timer** unit to execute the backup script on a regular schedule. By default, the timer is set to run **daily** (at midnight). This means once the role is applied, backups will occur automatically every day without manual intervention.
* **Backup Directory Management:** Ensures a dedicated backup directory exists on the host (defaults to `/var/backups/postgres`). If the directory is not present, the role creates it during execution. All backup files will be stored here. By centralizing backups in one folder, it’s easier to manage storage and apply uniform permissions or retention policies.
* **Lightweight Implementation:** The role uses native OS tools and minimal dependencies. It does **not** install heavy external software or daemons. It relies on the presence of PostgreSQL client utilities (primarily `pg_dump`) and standard compression tools. The backup script it installs (`/usr/local/bin/backup_postgres.sh`) is straightforward and can be run independently if needed. This design makes it easy to integrate with existing workflows and simplifies troubleshooting (the backup process is essentially one shell command).
* **Use Case Flexibility:** While designed to run on a PostgreSQL server host (dumping a local database), the role can be applied in various scenarios. For a standalone PostgreSQL server, you might use this role to schedule nightly backups of the main application database. In an environment with multiple databases or multiple servers, you could include this role for each database server (specifying the appropriate database to dump on each). The role is not tied to any specific application or schema – it simply backs up the database you point it at. This complements application-specific backup needs; for instance, you’d use **`backup_netbox`** for a NetBox app’s data, but use **`postgres_backup`** for other Postgres databases on your servers.

In summary, **Postgres Backup** provides an automated, on-host backup solution for PostgreSQL databases. Once configured, it continuously produces up-to-date dump files that can be used for disaster recovery, point-in-time snapshots, or data migration. By leveraging PostgreSQL’s own tools and Systemd’s scheduling, it avoids reinventing the wheel – offering a simple yet effective backup mechanism that system administrators can rely on.

## Supported Operating Systems/Platforms

This role is developed for and tested on **Debian-based** Linux systems (64-bit), specifically modern Debian and Ubuntu releases:

* **Debian** – 11 (Bullseye) and 12 (Bookworm)
* **Ubuntu** – 20.04 LTS (Focal Fossa) and 22.04 LTS (Jammy Jellyfish)

> **Note:** These Debian/Ubuntu distributions are the primary targets for **postgres\_backup**. The implementation uses Debian/Ubuntu conventions (e.g. the presence of `apt`, default system paths, the `postgres` user for service management). Other Linux distributions (such as RHEL, CentOS, AlmaLinux, etc.) are *not officially tested*. The role’s core functions (dumping a database and scheduling via Systemd) are OS-agnostic, but you may need to adjust certain details for non-Debian systems. For example, on an RPM-based distro you’d have to ensure PostgreSQL client utilities and gzip are installed via `yum/dnf`, and the default file paths or service names might differ. Use this role on non-Debian platforms with caution, and be prepared to modify variables (like package names or paths) and pre-install required packages as needed. Systemd is required on the target host; if your system uses a different init system, the service/timer setup would need to be adapted.

## Role Variables

Below is a list of important variables for this role, along with their default values (defined in **`defaults/main.yml`**) and descriptions:

<!-- markdownlint-disable MD033 -->

<details>
<summary>Role Variables (defaults)</summary>

| Variable         | Default Value             | Description |
| ---------------- | ------------------------- | ----------- |
| **`db_name`**    | `"postgres"`              | Name of the PostgreSQL database to back up. **Default is `"postgres"`**, which is the default maintenance database in PostgreSQL – you will almost certainly want to override this to the actual application database you need to back up. This variable tells the backup script which database to dump. |
| **`db_user`**    | `"postgres"`              | PostgreSQL username to connect as for the dump. Defaults to `"postgres"` (the PostgreSQL superuser), which typically has read access to all databases. You can set this to a lesser-privileged user (e.g. a dedicated backup user) if such a role exists and has access to the target database. Ensure the specified user has sufficient privileges to read the entire **`db_name`**. |
| **`backup_dir`** | `"/var/backups/postgres"` | Filesystem path to the directory where backup files will be stored. The role will create this directory if it doesn’t exist. By default it uses `/var/backups/postgres`, a common location for database backups on Debian systems. You may change this if you prefer a different path (for example, to use a mounted volume or another directory). Make sure the path is writable by the user performing the backup (root by default) and has enough space to store your dumps. |

</details>

<!-- markdownlint-enable MD033 -->

**Notes:** Typically, you **must override** at least `db_name` (and often `db_user`) to suit your environment. The defaults above use the PostgreSQL superuser and its default database as placeholders, mainly to ensure the script can run out-of-the-box. In practice, you’ll set `db_name` to your application’s database name (e.g. `"myappdb"`) and possibly use a less privileged `db_user` that you’ve created for backups. If using a custom backup user, you’ll also need to handle authentication (see **Known Issues and Gotchas** regarding database credentials). The `backup_dir` can remain at the default, but you can adjust it to align with your system’s backup storage strategy (for instance, pointing it to a specific backup mount or a different folder per environment). Always ensure the target directory has appropriate permissions and sufficient storage.

## Tags

This role does **not define any custom Ansible tags** for its tasks. All tasks in **postgres\_backup** run whenever the role is invoked, with no built-in way to skip subsets via tags. In other words, there are no role-specific tags like `postgres_backup_install` or `postgres_backup_config` – the entire backup setup runs as one unit.

*You can still use external tagging in your playbook if needed.* For example, you might include this role under a higher-level tag (say, `backup`) to control when all backup-related roles run. But within **postgres\_backup** itself, tasks are unconditionally executed (similar to many simple roles). This simplifies usage: include the role, and it will perform all steps needed to set up the backup. There is no partial execution mode provided by tags.

## Dependencies

* **Ansible Version:** This role has been used with Ansible **2.13+**. It relies only on Ansible’s built-in modules (`file`, `template`, `systemd`, etc.) and standard YAML syntax, so it doesn’t require cutting-edge Ansible features. Using Ansible 2.12 or later is recommended for full compatibility, but earlier 2.x versions might also work if they include the needed modules. Always run a quick test if using an older Ansible version.
* **Collections:** No special Ansible collections are required for core functionality. Unlike some other database backup roles, **postgres\_backup** does not use the `community.postgresql` collection’s modules for the actual dump (it uses a shell script with `pg_dump`). Thus, you do **not** need to pre-install that collection just for this role. (If you are using Postgres modules elsewhere in your playbook, for example in a provisioning role, then ensure the `community.postgresql` collection is installed – but it’s not a direct dependency of this backup role.)
* **External Packages:** The target host must have PostgreSQL client utilities and compression tools available:
  * **PostgreSQL client (for `pg_dump`):** The backup script calls `pg_dump` to perform the dump. On Debian/Ubuntu, the `pg_dump` binary is provided by the package **`postgresql-client`** (or by the server package if PostgreSQL is installed). In most cases, your database server already has this installed (especially if the server itself is running Postgres). If not, you should install the appropriate PostgreSQL client package before or as part of running this role. Without `pg_dump`, the backup script cannot function.
  * **Gzip:** The script uses **`gzip`** to compress the dump file. Nearly all Linux systems include gzip by default, but on minimal installations it’s possible it’s missing. The role will attempt to use it; ensure the `gzip` package is installed on the target node. (On Debian/Ubuntu, `gzip` is typically pre-installed or available via `apt`.)
* **Target Environment:** A running **PostgreSQL server** is assumed to exist on the host where this role is applied. The role does *not* install or configure PostgreSQL itself; it only handles the backup of an existing database. You should set up your database server beforehand (e.g., using a role like **geerlingguy.postgresql** or your own playbooks) and create the database(s) you intend to back up. In this repository’s context, for example, the Postgres server might be provisioned by an external role before `postgres_backup` is run.
* **Database User:** Ensure that the **`db_user`** you configure for backups exists in PostgreSQL and has adequate privileges on **`db_name`**. By default, the role uses the `postgres` superuser for convenience. If you use a different user (e.g., a dedicated backup role with lesser rights), you must create that user and grant it read access (SELECT) on all tables in the target database, plus the ability to run `pg_dump`. Typically, a user with the `pg_read_all_data` role (PostgreSQL 15+) or similar privileges can dump a database. The role does **not** create the database user for you. Set up the credentials as part of your DB provisioning.
* **Authentication Setup:** Because the backup is performed non-interactively, configure the authentication so that `pg_dump` can connect without manual intervention. This usually means one of:
  * Configuring **trusted local connections** (e.g., “peer” auth for the `postgres` user on Unix sockets, which is default on Debian) so that if `db_user` is `postgres` and the script runs as the `postgres` OS user, no password is needed.
  * OR setting up a **`.pgpass` file** for the user under which the backup runs (often root, see Security section) with the database credentials, so that `pg_dump -U <user>` will find the password and not prompt.
  * OR using environment variables like `PGUSER`/`PGPASSWORD` if you modify the service to supply those.

  The role itself does *not* handle these authentication details (no password prompt or vault integration is built-in), so you must ensure `pg_dump` will not hang waiting for a password. See **Known Issues and Gotchas** for more on this.
* **Systemd:** The host must be running Systemd (which is true for Debian 11+, Ubuntu 20.04+, and most modern Linux distros). The role installs a service unit file to `/etc/systemd/system/postgres_backup.service` and a timer to `/etc/systemd/system/postgres_backup.timer`. If Systemd is not present, these steps will fail. The role does not support other init systems out-of-the-box. If you are using a distribution with a different init (such as Alpine with OpenRC, or older CentOS with SysVinit), you would need to manually implement scheduling (for instance, via cron) or adapt the role accordingly.
* **Prior/Related Roles:** There are no hard dependencies declared in this role’s metadata. However, it is logical to use **postgres\_backup** in sequence with other roles. For example, you might run your PostgreSQL installation role first (to set up the server and create databases/users), then apply **postgres\_backup** to configure backups. In a multi-node environment (e.g., a primary/replica setup), you might choose to run the backup only on the primary or on a designated backup server. Coordination with other roles like a monitoring role (to watch backup status) or a maintenance role (to clean old files, if you implement that externally) can be considered as well.

## Example Playbook

Below is an example of how to use the `postgres_backup` role in an Ansible playbook. In this example, we assume a group of database hosts (`db_servers`) that are running PostgreSQL. We will apply the role to those hosts to install the backup script and schedule. We override the defaults to specify which database to back up and which user to use:

```yaml
- hosts: db_servers
  become: yes  # Ensure we have root privileges to deploy scripts and systemd units
  vars:
    db_name: "myappdb"          # The name of the database we want to back up
    db_user: "postgres"         # Using the Postgres superuser for dumping (adjust if using a custom user)
    backup_dir: "/var/backups/myappdb"  # Custom backup directory (ensure it exists or let the role create it)
  roles:
    - postgres_backup
```

In the above play, we target the `db_servers` group (adjust to your inventory structure; it could be a specific host or a different group name in your setup). We elevate to root (`become: yes`) because the role needs permission to write to system directories (like `/usr/local/bin` and `/etc/systemd/system`) and to start services.

We then provide the necessary variables:

1. `db_name` is set to `"myappdb"`, which is the database we want to back up. In your case, replace this with the actual database name you need to dump.
2. `db_user` is set to `"postgres"` in this example, meaning the backup will connect as the PostgreSQL superuser. (You could use a dedicated backup user here if you have one, e.g., `"backup_user"`. Just ensure that user can dump the database and handle authentication as needed.)
3. `backup_dir` is pointed to `/var/backups/myappdb` – a directory where we want the backup files to go. This directory will be created by the role if it doesn’t exist. You could omit this variable to use the default `/var/backups/postgres`, but here we customize it to separate backups per application.

When this play is run, the role will:

1. **Install Tools (if not already present):** *(This step is implicit – the role does not explicitly install packages, so ensure `pg_dump` and `gzip` are available as noted.)*
2. **Create Backup Directory:** It will create `/var/backups/myappdb` on the target host if that directory isn’t there already. The directory will be owned by root.
3. **Deploy Script:** Copy the `backup_postgres.sh` template to `/usr/local/bin/backup_postgres.sh` on the host. This shell script contains the `pg_dump` command and uses the variables we set (it will dump **myappdb** using the **postgres** user). The script is made executable (`0755`).
4. **Deploy Systemd Units:** Place the `postgres_backup.service` unit file in `/etc/systemd/system/` and the `postgres_backup.timer` unit in the same directory. The service unit is a oneshot that runs the backup script, and the timer is configured to trigger the service daily.
5. **Reload Systemd:** Inform Systemd about the new unit files (daemon-reload).
6. **Enable & Start Timer:** Enable the timer to start on boot and start it immediately. The timer will be active and, according to its schedule, will invoke the service. (By default, `OnCalendar=daily` means it will run shortly after midnight each day. If you start the timer during the day, it might also trigger immediately to catch up if `Persistent=true` – meaning if the system was down at the scheduled time, it runs as soon as possible.)

After the play, on each database host, you should see the backup directory populated with at least one backup file (since we started the timer, it usually triggers the service right away once enabled). For example, you might find a file like `/var/backups/myappdb/myappdb-2025-06-02.sql.gz` (with the date in the filename). You can list the directory or check `journalctl -u postgres_backup.service` on the host to confirm that the backup ran successfully. In practice, once this setup is in place, you have a daily automated backup – just ensure to monitor it and manage the stored files as needed (cleanup and offsite copy are addressed below).

**Usage in Practice:** Typically, you would include this role in a larger playbook for your database server(s). It’s common to run it after the database is installed and configured. Note that if your inventory or other roles already provide the necessary variables (for example, you might have `db_name` and `db_user` set as group vars for your DB servers), you might not need to set them in the playbook explicitly. The example shows them for clarity. Also, remember to handle database credentials (if needed) through safe means – for instance, if your `db_user` requires a password, you might store that in an Ansible Vault and ensure a `.pgpass` file is set up on the host or use the `become_user: postgres` approach (more on that in **Known Issues and Gotchas**). The above playbook doesn’t address that for simplicity.

## Testing Instructions

It’s important to test this role in an isolated environment before using it in production, to verify that it performs as expected with your specific PostgreSQL setup. The recommended way to do this is using **Molecule** (with the Docker driver) for automated role testing. Below are steps to set up and run a test:

1. **Prepare Testing Tools:** Install Molecule and Docker on your development machine. You can install Molecule (and its Docker support) via pip:

   ```bash
   pip install molecule[docker]
   ```

   Ensure that Docker is installed and running, as Molecule will create containers for testing.
2. **Initialize a Test Scenario:** If the `postgres_backup` role already includes a Molecule test scenario (check for a `roles/postgres_backup/molecule/` directory), you can use that. If not, you can create a new scenario yourself:

   ```bash
   molecule init scenario -r postgres_backup -d docker
   ```

   This will generate a `molecule/` directory with a default scenario (usually named “default”) for the role. It includes a base Molecule config and an example playbook (`converge.yml`) that applies the role.
3. **Customize the Test Scenario:** Edit the Molecule configuration (`molecule/default/molecule.yml`) and the `converge.yml` playbook as needed:
   * Use an appropriate Docker base image. For Debian-based testing, you might use an image like `geerlingguy/docker-debian11-ansible` (which has systemd enabled for roles that need it) or `ubuntu:focal` plus steps to enable systemd. The image should be similar to your target OS (Debian 11, Ubuntu 20.04, etc.).
   * In the `converge.yml`, ensure that before the `postgres_backup` role runs, the container has PostgreSQL and required tools. You might need to add tasks to install `postgresql` (server or client) and `gzip`. For example, you could use the `geerlingguy.postgresql` role in the test, or simply add tasks to install `postgresql-client` and `postgresql` packages via apt, then initialize a database for testing. At minimum, having `pg_dump` available is necessary. You’ll also want to create a dummy database to dump (and perhaps a table with sample data) so you can verify the dump contents.
   * If your test uses the `postgres` user for dumping (default), and the container’s PostgreSQL is fresh, you might be able to rely on peer authentication for the `postgres` user. Otherwise, you might configure a password for a test user and adjust authentication or use a `.pgpass` in the container. This can get complex, so using the `postgres` user with peer auth is simplest for a quick test.
4. **Run the Role in the Test Container:** Execute Molecule to run the converge step:

   ```bash
   molecule converge
   ```

   Molecule will build the Docker container, apply the tasks (including installing Postgres if you configured that, and then running `postgres_backup` role). Watch the output for any errors. If everything is configured correctly, the role’s tasks should complete without failures. The Systemd timer will be started in the container.
5. **Verify Backup Artifacts:** After convergence, you can check that the backup was created inside the container:

   ```bash
   docker exec -it <container_id> ls -l /var/backups/postgres
   ```

   (Replace `<container_id>` with the Molecule container ID or name; you can find it with `docker ps` and looking for the one Molecule created.) You should see at least one file, e.g., `postgres-<DATE>.sql.gz` or whatever `db_name` you used in the test. You might also inspect the journal or status:

   ```bash
   docker exec -it <container_id> systemctl status postgres_backup.service
   ```

   to see logs from the last backup run. If you included multiple databases or changed the schedule for testing, verify those as well (you can manually trigger the timer with `systemctl start postgres_backup.service` inside the container to force a backup).
6. **Optional - Verify Contents:** If you want to be thorough, you can copy the backup file out of the container (using `docker cp`) and then restore it to a test database to ensure it’s valid:

   ```bash
   docker cp <container_id>:/var/backups/postgres/postgres-2025-06-02.sql.gz ./testbackup.sql.gz
   gunzip testbackup.sql.gz
   psql -U postgres -d <newdb> -f testbackup.sql
   ```

   This will require a running PostgreSQL instance on your machine or another container. The goal is to ensure the SQL file is not corrupted and can recreate the database. For routine tests, this is usually not necessary unless you suspect issues.
7. **Automated Tests (Optional):** If you wrote any Molecule verify tests (for example, using Testinfra or asserting file existence in Ansible), you can run:

   ```bash
   molecule verify
   ```

   to execute them. In many cases for backup roles, manual verification is sufficient – e.g., checking that the file exists and maybe that its size is > 0, etc., can be automated.

By following the above steps, you can confidently test the **postgres\_backup** role in a controlled environment. This helps iron out any issues with permissions, environment, or configuration before you apply the role to actual servers. It’s especially useful to test the database connection and dump process, as those can vary depending on authentication setup. Adjust the Molecule scenario as needed to match your production setup (for instance, if your production uses a specific non-root user to run the backup, simulate that in the container). Once the Molecule tests pass and you have the expected backup file, you can proceed to use the role in your infrastructure, knowing that it has been validated.

## Known Issues and Gotchas

* **Database Credentials & Authentication:** One of the most important considerations is that this role does **not** handle providing the database password to `pg_dump`. If your PostgreSQL setup requires a password for the `db_user` (as is common for non-`postgres` users or when not using peer authentication), the backup script will hang waiting for a password, or fail. By default, if you use the `postgres` Linux user and the `postgres` DB role on Debian-based systems, peer auth allows passwordless dumps (the role’s default scenario). However, for any other user or remote connection, you must set up non-interactive auth. The recommended approach is to use a [**`.pgpass`** file](https://www.postgresql.org/docs/current/libpq-pgpass.html) on the host. For example, for a backup user, create `/root/.pgpass` (since the service runs as root by default) with a line containing host, port, database, username, and password, and set its permissions to `0600`. Then `pg_dump` will read the password from there and not prompt. Alternatively, you could modify the Systemd service to run as the `postgres` OS user (adding `User=postgres` in the unit file) so that peer authentication is used for `db_user = postgres` – this would avoid the need for a password, but you must ensure the backup directory permissions allow the `postgres` user to write to it. In short, **plan for how the backup script will authenticate to the database**. This role leaves that to you: either use the local superuser with peer auth, or configure credentials securely. Do not hardcode passwords in the script or playbook (and the role doesn’t provide a `db_password` variable for this purpose by design).
* **Backup Schedule Timing:** The Systemd timer is set to run *daily at midnight* by default. “Daily” in Systemd corresponds to one invocation each day at 00:00 local time. This might not be ideal for all environments – for instance, if your database is heavily used at midnight or if multiple servers all dump at the same time, you might prefer a different schedule. At present, the role does not expose a variable to change the timer’s schedule (the value is hardcoded in the template). If you need to adjust it, you have a couple of options: (1) After running the role, manually edit the `/etc/systemd/system/postgres_backup.timer` file on the host to the desired schedule (e.g., change `OnCalendar` to `"02:00"` for 2 AM, or to `weekly` for weekly backups) and then run `systemctl daemon-reload && systemctl restart postgres_backup.timer`. This manual step will override the default. (2) Alternatively, you could fork/modify the role to parameterize the schedule. Keep in mind that if you re-run the unmodified role, it will reinstall the original timer definition, so document any manual changes. Also, note that the timer uses `Persistent=true`, meaning if a scheduled run is missed (e.g., the machine was off at midnight), it will run as soon as possible after boot to catch up.
* **Single-Database Focus:** This role backs up **one PostgreSQL database** per run (the one defined by `db_name`). It does not automatically loop over all databases on the server. If you have multiple databases that require backups, you’ll need to apply this role separately for each database or use multiple timer-service pairs. For example, you could run `postgres_backup` role once with `db_name=app1` (and perhaps use a different `backup_dir`), and again with `db_name=app2`. Another approach is to modify the script to use `pg_dumpall` or to accept an environment variable listing multiple DBs. Out-of-the-box, it’s one database = one backup file. This keeps the role simple and targeted, but it’s a consideration in environments like shared database servers. Ensure you don’t forget to back up any important database. If you only run the role for a single DB but the server has others, those others won’t be captured.
* **Retention (Old Backups):** This role **does not automatically prune or rotate old backups**. Each execution of the backup will create a new `.sql.gz` file in the backup directory, and none are removed by the role. Over time, these files can accumulate and consume significant disk space. It is up to you to implement a retention policy. You can manage this in a few ways:
  * Manually or via a cron job, periodically delete files older than a certain number of days. For instance, you might run a cron task to execute something like:
    ```bash
    find /var/backups/postgres -type f -name "*.sql.gz" -mtime +14 -delete
    ```
    to remove backup files older than 14 days. Adjust the path and age as needed (and test your find command to avoid deleting wrong files).
  * Integrate cleanup into an Ansible playbook that runs occasionally. You could write an Ansible task to do the same find & delete or use the [`ansible.builtin.find`](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/find_module.html) module combined with `file remove` to purge old backups.
  * Use logrotate or a similar mechanism by treating the backup directory as logs – though logrotate is not typically for SQL dumps, it can be configured to rotate and keep N files.
  Whichever method, define a policy (e.g., keep last 7 daily backups, or keep 4 weekly backups, etc.) and implement it. Regularly monitor the backup storage. **Do not let backups fill up your disk** – that could not only cause backup failures but also impact the database or other services if the partition is full.
* **Offsite Copying:** The backups created by this role remain on the database host itself (in `backup_dir`) unless you take additional steps to move them elsewhere. Storing backups on the same machine as the database can be risky – if that server fails or its disk is lost, the backups go with it. It’s crucial to have an offsite/off-server backup strategy. This role doesn’t implement any remote transfer, but here are some suggestions:
  * Use an Ansible **fetch** task or **synchronize** module in a higher-level playbook, after this role runs, to pull the backup file to the control node or push it to a storage server.
  * Set up a separate “backup server” that mounts the backup directory via NFS/SMB or uses rsync/scp to copy files at intervals.
  * Use cloud storage or backup services: for example, run an AWS CLI or rclone command (could be triggered via an Ansible task or cron job) to upload the `.sql.gz` file to S3 or another cloud storage, after each backup.
  * If you’re using Ansible AWX/Controller, you might schedule a job that runs this role and then uses the **copy module** to copy files to a network location.
  Choose a method that fits your ops model and test it. The key point is: **having the backup only on the source host is not sufficient for disaster recovery**. Always get your backups onto a separate system or medium.
* **Consistency and DB State:** PostgreSQL’s `pg_dump` produces a **consistent snapshot** of the database at the time the dump begins, without blocking other operations significantly. This means you can run backups while the database is online, and you won’t get a half-committed transaction in the dump. Normal writes to the database can continue during the backup (they won’t be included if they commit after the snapshot). For most cases, this “online backup” is acceptable and the resulting dump is logically consistent. However, be aware of a couple of scenarios:
  * DDL changes (schema changes) occurring during the dump might cause `pg_dump` to fail or produce warnings if, say, a table is dropped or altered while dumping. This is rare in stable production environments, but possible if you deploy migrations concurrently with a backup. It’s usually best not to schedule structural changes at the exact backup time, or to use `--schema` flags to limit what to dump if needed.
  * If your environment has extremely high write volume, the dump might put some load on the database (as it reads a lot of data). The impact is generally moderate because `pg_dump` uses a single transaction and doesn’t lock tables in a way that blocks writes (except very briefly). Still, for very large databases, consider the performance impact – it may be advisable to run backups during off-peak hours to minimize any potential effect on users.
  * The role does *not* stop PostgreSQL or do anything to guarantee an application-level quiescence. It’s purely an online logical backup. If you require a point-in-time recovery or zero-downtime backups beyond what `pg_dump` provides (for example, capturing write-ahead logs for PITR, or using snapshotting filesystems for a physical backup), those are outside the scope of this role. You may need a more advanced backup setup (like continuous archiving, `pg_basebackup`, or third-party tools) for that level of consistency. For most use cases, a nightly `pg_dump` is a good balance of consistency and simplicity.
* **Restoration Process:** It’s worth noting that restoring from these backups is a **manual** process (the role does not include a restore counterpart). In the event you need to restore, you would take a backup file (e.g., `myappdb-2025-06-02.sql.gz`), transfer it to a PostgreSQL server (if not already there), and run a command to import it. A basic restore procedure is:
  1. Stop or isolate the application that uses the database (to avoid confusion or conflicts during restore).
  2. Create a new empty database (unless you plan to overwrite the existing one). For example: `createdb myappdb_restored` on the server.
  3. Decompress the backup: `gunzip myappdb-2025-06-02.sql.gz` yielding `myappdb-2025-06-02.sql`.
  4. Load the dump into the database: `psql -U <appropriate_user> -d myappdb_restored -f myappdb-2025-06-02.sql`. Use a superuser or a user with permissions to create all schema objects.
  5. Reconfigure your application to point to the restored DB if needed, or swap it in place of the old one.
  Always test your backups by performing trial restores on a staging or dev environment. This practice ensures that the backups are valid and that you are familiar with the restore steps **before** an actual emergency. A backup is only as good as your ability to use it for recovery. Document your restore procedure as part of your disaster recovery plan.
* **Multiple Nodes / Replication Considerations:** If you are running a PostgreSQL replication setup (primary/standby), it’s usually unnecessary to back up from every node. In fact, doing so simultaneously could be wasteful and put extra load on the cluster. Commonly, you would run backups on the **primary server** or on a designated backup replica. For example, if you have a hot standby, you might run the dump there to offload the read workload from the primary – but ensure that the standby is truly read-only and in sync. The `postgres_backup` role can be targeted to whichever host you choose. If you do run it on multiple nodes, consider using Ansible’s `serial` directive to stagger them, so that not all nodes dump at once (e.g., `serial: 1` for the task or role in your play, which will do one at a time). Also note, if you dump the same database from a primary and a standby, you’ll get essentially identical content (assuming the standby is up-to-date), so one copy is usually sufficient. Plan your backup strategy accordingly to avoid redundant work and to minimize impact on your database performance.

## Security Implications

* **Privilege Usage:** This role performs actions that require elevated privileges on the target system. All tasks are run with `become: true` (typically as **root**), and the deployed Systemd service is, by default, run as root as well (since no explicit user is set in the service unit, Systemd will run it as root). This is done for practicality – root can read the database data (if using socket peer auth as `postgres` user) and write to any directory. However, running backups as root means that if someone were to compromise the backup script or the execution, they have high privileges. Ensure that only trusted administrators have access to run or modify this role and that your Ansible environment is secure. If desired, you can lower the privilege by editing the service to run as the `postgres` OS user (or another dedicated user), but then make sure file permissions (on the backup directory and script) are adjusted so that user can access them.
* **Storage of Credentials:** As noted, the role doesn’t include a password variable; it relies on external methods for authentication. Avoid storing database passwords in plain text on disk or in your playbooks. If you use a `.pgpass` file for the root user, that file contains the password in plain text – restrict its permissions to `600` and keep it only on the host (don’t commit it to any repo). If you use Ansible Vault or another secrets manager to distribute the password, that’s fine, but the moment of use will be either in `.pgpass` or environment variables. Treat those secrets carefully. Do not echo passwords in logs. Essentially, consider the database credentials as sensitive as any other production secret and handle accordingly. This role defers to you on how to inject the secret for `pg_dump` to use (if needed), so implement that with security in mind.
* **Backup File Sensitivity:** The dump files produced by this role contain the **entire contents of your database** (for the specified `db_name`). This often includes sensitive or confidential information: user data, account credentials (hashed passwords), configuration, perhaps even secrets stored in the DB. **Anyone with read access to these files essentially has all that data.** By default, the role-created backup directory will be owned by root, but because we did not explicitly set permissions in the role, it likely ends up with the system default (on many systems, that’s `drwxr-xr-x` or `0755` for directories created by root). Similarly, the dump file created via the script will have default permissions (usually `0644`, meaning readable by all users). This is not ideal from a security standpoint. It means unprivileged users on the system could list and read the backup unless you lock it down. We **strongly advise tightening the permissions** on the backup directory *after the role runs*. For instance, you can manually `chmod 0750 /var/backups/postgres` and even set the group ownership to a restricted group. That way, only root (and maybe members of an admin group) can access the files. Another approach is to modify the role or script to set a umask (e.g., `077`) so files are created with no world access. The key point: ensure the backup files are not world-readable. On a dedicated database server with no untrusted local users, this might be less of a concern, but on any multi-user system it’s critical.
* **Encryption of Backups:** The role does not encrypt the SQL dump files – they are plain text (inside a gzip). If your policy or threat model requires that backups at rest be encrypted (for example, in case the backup files are stolen or an old disk is disposed of), you should incorporate encryption into the process. This is not handled by default, but you could extend the script or add a post-processing step. For instance, you might pipe the output to GnuPG to encrypt with a public key, or use OpenSSL to symmetrically encrypt the `.gz` file. Even something like creating an encrypted filesystem at `/var/backups` could work. The **NetBox Enterprise** guide, for example, recommends encrypting backup files as part of the process. While that’s outside the scope of this role, be aware of the option. If you add encryption, make sure key management is handled (public-key encryption is preferred so you’re not embedding passwords). Do not rely on just filesystem permissions if the data is extremely sensitive – encryption adds another layer of defense if backups are exfiltrated.
* **Network Exposure:** The operations performed by this role do not open any network ports or create any network listeners on the host. Backups are done locally via the Postgres socket or localhost TCP (depending on your configuration). However, if you implement offsite transfer (as discussed above), use secure methods. For example, if using SCP/SFTP to copy files, use SSH keys with passphrase or proper permissions. If using a cloud upload, ensure the connection is encrypted (HTTPS) and the credentials to the cloud are protected (maybe use IAM roles or short-lived tokens rather than embedding API keys in plain text). In short, the role itself doesn’t increase network attack surface, but any extension you do to move backups around should follow security best practices.
* **System Changes and Footprint:** This role adds a few files to the system (the script and systemd units) and starts a timer. It doesn’t create any new system users or install heavy software. The script is simple and only calls out to `pg_dump` and `gzip`. From a system integrity perspective, it’s low-impact. Still, be mindful that running anything as root on a schedule means you should trust the content. If you or others modify the `backup_postgres.sh` script template, review changes carefully – a mistake like pointing `backup_dir` to the wrong path could cause the script to write a large file to an unintended location, etc. The default settings are safe and sane for most cases. As always, keep your servers updated (so that any security issues in `pg_dump` or related tools are patched) and monitor your backups.
* **Log Visibility:** By default, the output of the backup script (if any) will go to the journal since it’s run by systemd. If a backup fails, Systemd will record the failure and log the output of `pg_dump` (error messages) to the journal, which you can view with `journalctl -u postgres_backup.service`. You might want to set up an alert or monitoring on backup failures or missing files. Consider configuring your monitoring system to check that new backup files appear daily and/or that the systemd timer hasn’t reported errors. Security-wise, be cautious if you output sensitive info in the backup process (the default script does not output the password or data, only errors if any). It’s generally safe, but just be aware logs could contain error messages from `pg_dump` which might include some info about the connection or database state if things go wrong. Keep your logs protected as per normal operations.

By understanding and addressing these considerations, you can safely run the **postgres\_backup** role in your environment. The goal is to protect your data without introducing new vulnerabilities. As always, balance convenience with security: an automated backup is great for reliability, but it must be configured securely to truly serve its purpose in a crisis.

## Cross-Referencing

The **postgres\_backup** role is one component of a broader toolkit of roles and practices for database and application maintenance. Within this repository and in the Ansible ecosystem, you may find the following roles and references useful in conjunction with or as alternatives to **postgres\_backup**:

* **PostgreSQL Server Setup:** This repository typically relies on an external role like **`geerlingguy.postgresql`** to install and configure PostgreSQL on servers. If you are looking to set up the PostgreSQL database itself (users, databases, tuning parameters, etc.), consider using that role or a similar one. Ensure that the database server is configured (and database created) before applying `postgres_backup`. The backup role assumes an operational Postgres instance to work with.
* **NetBox Backup (`backup_netbox` role):** For backing up the NetBox application’s data, including its PostgreSQL database and media files, the **`backup_netbox`** role is provided in this repository. It is a more application-specific backup solution. Notably, it uses Ansible modules to dump the NetBox DB and handles media file archiving, rather than setting up a persistent service. If you are running NetBox, you would use `backup_netbox` to handle that app’s backups (which under the hood also dumps the Postgres database, but with NetBox-specific context). The `backup_netbox` documentation cross-references this **postgres\_backup** role as a general solution for other databases. In short: use **postgres\_backup** for generic Postgres backups, and **backup\_netbox** for the NetBox app (which is a special case combining DB and file backup).
* **MariaDB/MySQL Backups (`mariadb_backups` role):** If your infrastructure includes MySQL/MariaDB databases, the analogous role **`mariadb_backups`** in this repository can be used. It has a similar philosophy (dump databases with `mysqldump` and compress) but is tailored to MariaDB/MySQL specifics. It installs a script and service for MySQL backups. The cross-reference is that both the Postgres and MariaDB backup roles aim for parity – giving you a consistent way to back up different types of databases. Administrators managing heterogeneous environments can use both roles to ensure all databases (Postgres or MySQL family) are backed up in a similar automated manner.
* **Application-Specific Backup Roles:** Aside from NetBox, there are other backup roles in this repository for specific services. For example, **`jenkins_backup`** is a role that handles backing up Jenkins data (jobs, configs, etc.). Unlike `postgres_backup`, which focuses purely on a database, `jenkins_backup` deals with file system archiving and has its own retention logic. If you manage Jenkins, that role can complement database backups by safeguarding your CI configuration. Similarly, check if there are roles for other stateful services you use (e.g., perhaps a role for backing up an LDAP directory or file server). Using the suite of backup roles together ensures all parts of your stack are covered (databases, applications, configs).
* **“Base” or Common Roles:** Prior to running a backup role on a fresh server, you might run a baseline role such as **`common`** or **`base`** (names can vary) that preps the system. In this repository, the **`common`** role handles basic setup like apt updates, installing Python and other utilities. Including such a role in your playbook before `postgres_backup` is a good idea – it can ensure `gzip` is installed, the system time is synced (important for correct timestamps on backups), and that any other baseline security hardening is in place. While not directly related to database backups, a well-configured system (with NTP, updated packages, proper locale, etc.) creates a smoother experience for running backups.
* **Disaster Recovery Planning:** Beyond Ansible roles, remember to maintain documentation and procedures for disaster recovery. A role like **postgres\_backup** is a tool to help implement your backup strategy, but you should have documentation on when and how to use the backups. Cross-reference your organization’s DR plan or runbooks. For example, ensure there is a document that says “To restore the database, use the latest .sql.gz from server X, and follow these steps…”. Tie that into the outputs of this role. Perhaps store the backup inventory (list of backup files and dates) in a centralized location or monitoring system. While this is not a specific Ansible role, it’s a best practice to connect the technical implementation (this role) with policy and planning.

Each of the above roles and practices complements **postgres\_backup**. By combining them appropriately, you can achieve a robust backup and recovery setup. For instance, a full deployment might involve: setting up PostgreSQL with `geerlingguy.postgresql`, deploying your app, running `postgres_backup` for the DB, running `jenkins_backup` for Jenkins, using `backup_netbox` for NetBox, and so on, all coordinated via your playbooks. The modular nature of these roles allows you to include only what you need. Be sure to read each role’s README for specifics, just as you’re reading this one – they contain valuable details and caveats.

---

By following this documentation and using the role as described, you can automate PostgreSQL backups in a consistent, secure manner. Remember to monitor the backups, test your restores, and adjust the configuration as your environment grows. The **postgres\_backup** role is meant to simplify the technical steps of dumping and scheduling; it’s up to you to integrate it into a comprehensive data protection strategy. Good luck, and happy backing up!
